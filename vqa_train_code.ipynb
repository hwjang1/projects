{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GPU를 사용할 수 있는 환경이라면 CUDA 관련 세팅을 처리 후 사용 패키지를 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm \n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DATA 전처리에 사용할 Pytorch 데이터세트 코드를 선언합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenizer, data, answer_list, max_token, transform=None):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_token = max_token\n",
    "        self.answer_list = answer_list        \n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        question = self.data['question'][index] #질문\n",
    "        answer = self.data['answer'][index]  #응답\n",
    "        img_loc = self.data['image'][index] #사진파일\n",
    "        \n",
    "        #BERT기반의 Tokenizer로 질문을 tokenize한다.\n",
    "        tokenized = self.tokenizer.encode_plus(\"\".join(question),\n",
    "                                     None,\n",
    "                                     add_special_tokens=True,\n",
    "                                     max_length = self.max_token,\n",
    "                                     truncation=True,\n",
    "                                     pad_to_max_length = True)\n",
    "        \n",
    "        \n",
    "        #BERT기반의 Tokenize한 질문의 결과를 변수에 저장\n",
    "        ids = tokenized['input_ids']\n",
    "        mask = tokenized['attention_mask']\n",
    "        image = Image.open(img_loc).convert('RGB')  #이미지 데이터를 RGB형태로 읽음 질문을 tokenize한다.\n",
    "        image = self.transform(image)  #이미지 데이터의 크기 및 각도등을 변경\n",
    "        \n",
    "        answer_ids = self.answer_list[self.answer_list['answer']==answer].index #응답을 숫자 index로 변경, e.g.) \"예\"-->0 \"아니요\" --> 1\n",
    "        if len(answer_ids)==0:\n",
    "            answer_ids = self.answer_list[self.answer_list['answer']==\"예\"].index\n",
    "\n",
    "        #전처리가 끝난 질의, 응답, 이미지 데이터를 반환\n",
    "        return {'ids': torch.tensor(ids, dtype=torch.long), \n",
    "                'mask': torch.tensor(mask, dtype=torch.long),\n",
    "                'answer': torch.tensor(answer_ids, dtype=torch.long),\n",
    "                'image': image}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 학습모델의 질문 및 이미지 처리에 대한 처리 모델 코드를 선언합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQAModel(nn.Module):\n",
    "    def __init__(self, num_target, dim_i, dim_q, dim_h=1024):\n",
    "        super(VQAModel, self).__init__()\n",
    "        \n",
    "        #The BERT model: 질문 --> Vector 처리를 위한 XLM-Roberta모델 활용\n",
    "        self.bert = transformers.XLMRobertaModel.from_pretrained('xlm-roberta-base')\n",
    "        \n",
    "        #Backbone: 이미지 --> Vector 처리를 위해 ResNet50을 활용\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, dim_i)\n",
    "        self.i_relu = nn.ReLU()\n",
    "        self.i_drop = nn.Dropout(0.2)\n",
    "        \n",
    "        #classfier: MLP기반의 분류기를 생성\n",
    "        self.linear1 = nn.Linear(dim_i, dim_h)\n",
    "        self.q_relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(dim_h, num_target)\n",
    "        self.q_drop = nn.Dropout(0.2)\n",
    "        \n",
    "        \n",
    "    def forward(self, idx, mask, image):\n",
    "        \n",
    "        _, q_f = self.bert(idx, mask) #질문을 Bert를 활용해 Vector화\n",
    "        i_f = self.i_drop(self.resnet(image)) # 이미지를 resnet을 활용해 Vector화\n",
    "        \n",
    "        uni_f = i_f*q_f #이미지와 질문 vector를 point-wise연산을 통해 통합 vector생성\n",
    "\n",
    "        return self.linear2(self.q_relu(self.linear1(uni_f))) #MLP classfier로 답변 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 학습과, 테스트를 수행하기 위한 코드를 선언합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, train_loader, criterion, optimizer, device):\n",
    "    \n",
    "    total_count_correct = 0\n",
    "    total_num_example = 0\n",
    "    total_loss = []\n",
    "\n",
    "    model.train()\n",
    "    for idx, batch in tqdm(enumerate(train_loader), total=len(train_loader), leave=False): #학습 데이터를 batch size만큼씩 읽어옴\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        imgs = batch['image'].to(device)  #이미지\n",
    "        q_bert_ids = batch['ids'].to(device) #질문\n",
    "        q_bert_mask = batch['mask'].to(device) \n",
    "        answers = batch['answer'].to(device) #응답\n",
    "        answers = answers.squeeze()\n",
    "        \n",
    "        outputs = model(q_bert_ids, q_bert_mask, imgs) #모델에 이미지, 질문, 응답을 넣음\n",
    "        loss = criterion(outputs, answers) #예측된 답변과 실제 정답과 비교하여 lossr계산\n",
    "\n",
    "        loss.backward(loss)\n",
    "        optimizer.step()\n",
    "        \n",
    "        predicted = torch.argmax(outputs, dim=1)\n",
    "        count_correct = np.count_nonzero((np.array(predicted.cpu())==np.array(answers.cpu())) == True) #정답갯수를 계산\n",
    "        total_count_correct += count_correct\n",
    "        total_num_example += answers.size(0)\n",
    "        total_loss.append(loss.item())    \n",
    "    print(\"TRAIN LOSS:\", str(sum(total_loss)/total_num_example) + \" Accuracy: \" + str(total_count_correct/total_num_example))\n",
    "    \n",
    "def test_fn(model, test_loader, data_frame, device):\n",
    "    total_count_correct = 0\n",
    "    total_num_example = 0\n",
    "    total_loss = []\n",
    "    model.eval()\n",
    "    \n",
    "    for idx, batch in tqdm(enumerate(test_loader), total=len(test_loader), leave=False):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        imgs = batch['image'].to(device)\n",
    "        q_bert_ids = batch['ids'].to(device)\n",
    "        q_bert_mask = batch['mask'].to(device)\n",
    "        answers = batch['answer'].to(device)\n",
    "        answers = answers.squeeze()\n",
    "\n",
    "        outputs = model(q_bert_ids, q_bert_mask, imgs) #모델에 이미지, 질문, 응답을 넣음\n",
    "        loss = criterion(outputs, answers) #예측된 답변과 실제 정답과 비교하여 lossr계산\n",
    "\n",
    "        loss.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted = torch.argmax(outputs, dim=1)  #예측된 정답  \n",
    "        count_correct = np.count_nonzero((np.array(predicted.cpu()) == np.array(answers.cpu())) == True)\n",
    "        total_count_correct += count_correct\n",
    "        total_num_example += answers.size(0)\n",
    "        total_loss.append(loss.item())\n",
    "            \n",
    "    print(\"TEST LOSS:\", str(sum(total_loss) / total_num_example) + \" Accuracy: \" + str(total_count_correct / total_num_example))\n",
    "            \n",
    "def answering(model, img_file, question, tokenizer, train_answer, device):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((356, 356)),\n",
    "            transforms.RandomCrop((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    img = transform(Image.open(img_file).convert(\"RGB\")).unsqueeze(0)\n",
    "    img = img.to(device)\n",
    "    encoded = tokenizer.encode_plus(\"\".join(question),\n",
    "                                    None,\n",
    "                                    add_special_tokens=True,\n",
    "                                    max_length=30,\n",
    "                                    truncation=True,\n",
    "                                    pad_to_max_length=True)\n",
    "\n",
    "    ids, mask = encoded['input_ids'], encoded['attention_mask']\n",
    "    ids = torch.tensor(ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    mask = torch.tensor(mask, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    output = model(ids, mask, img) #모델에 이미지, 질문, 응답을 넣음\n",
    "    predicted = torch.argmax(output, dim=1).item()\n",
    "    return train_answer['answer'].iloc[predicted]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 학습에 사용할 json 데이터를 로드하여 모델에 맞도록 가공 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL_FILE = './model.tar'\n",
    "MODEL_FILE = None\n",
    "\n",
    "if MODEL_FILE is not None:\n",
    "    checkpoint = torch.load(MODEL_FILE)\n",
    "    train_df = checkpoint[\"train_df\"]\n",
    "    answer_list = checkpoint[\"answer_list\"]\n",
    "    model = VQAModel(num_target=len(answer_list), dim_q=768, dim_i=768, dim_h=1024)\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    model = model.to(DEVICE)    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.00002)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])    \n",
    "    \n",
    "else:\n",
    "    with open('dataset/v1_OpenEnded_NIA_trainset_images.json') as json_file:\n",
    "        train_image_json = json.load(json_file)\n",
    "    with open('dataset/v1_OpenEnded_NIA_trainset_question.json') as json_file:\n",
    "        train_question_json = json.load(json_file)\n",
    "    with open('dataset/v1_OpenEnded_NIA_trainset_annotation.json') as json_file:\n",
    "        train_annotation_json = json.load(json_file)\n",
    "    train_image_df = pd.DataFrame(train_image_json['images'])\n",
    "    train_question_df = pd.DataFrame(train_question_json['questions'])\n",
    "    train_annotation_df = pd.DataFrame(train_annotation_json['annotations'])\n",
    "    \n",
    "    train_df = pd.merge(train_image_df, train_question_df)\n",
    "    train_df = pd.merge(train_df, train_annotation_df)\n",
    "    train_df.rename(columns={'multiple_choice_answer': 'answer'}, inplace=True)\n",
    "    train_df['image'] = train_df['image'].apply(lambda x : 'dataset/train_images/' + x)    \n",
    "    \n",
    "    train_answer = train_df['answer'].value_counts().reset_index()\n",
    "    train_answer.columns=['answer', 'count']\n",
    "    \n",
    "    model = VQAModel(num_target=len(train_answer), dim_q=768, dim_i=768, dim_h=1024)\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    model = model.to(DEVICE)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.00002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 테스트에 사용할 json 데이터를 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/v1_OpenEnded_NIA_testset_images.json') as json_file:\n",
    "    test_image_json = json.load(json_file)\n",
    "with open('dataset/v1_OpenEnded_NIA_testset_question.json') as json_file:\n",
    "    test_question_json = json.load(json_file)\n",
    "with open('dataset/v1_OpenEnded_NIA_testset_annotation.json') as json_file:\n",
    "    test_annotation_json = json.load(json_file)\n",
    "test_image_df = pd.DataFrame(test_image_json['images'])\n",
    "test_question_df = pd.DataFrame(test_question_json['questions'])\n",
    "test_annotation_df = pd.DataFrame(test_annotation_json['annotations'])\n",
    "\n",
    "test_df = pd.merge(test_image_df, test_question_df)\n",
    "test_df = pd.merge(test_df, test_annotation_df)\n",
    "test_df.rename(columns={'multiple_choice_answer': 'answer'}, inplace=True)\n",
    "test_df['image'] = test_df['image'].apply(lambda x : 'dataset/test_images/' + x)    \n",
    "test_df = test_df[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 학습 및 테스트에 사용될 데이터를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>image</th>\n",
       "      <th>question_id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>dataset/train_images/NIA_dataset03_00000000000...</td>\n",
       "      <td>1000</td>\n",
       "      <td>이것은 무슨 용도입니까?</td>\n",
       "      <td>알 수 없음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>dataset/train_images/NIA_dataset03_00000000000...</td>\n",
       "      <td>1001</td>\n",
       "      <td>테이블의 색깔은 무슨 색입니까?</td>\n",
       "      <td>예</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>dataset/train_images/NIA_dataset03_00000000000...</td>\n",
       "      <td>1002</td>\n",
       "      <td>그린 꽃은 몇 송이입니까?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id                                              image  question_id  \\\n",
       "0         1  dataset/train_images/NIA_dataset03_00000000000...         1000   \n",
       "1         1  dataset/train_images/NIA_dataset03_00000000000...         1001   \n",
       "2         1  dataset/train_images/NIA_dataset03_00000000000...         1002   \n",
       "\n",
       "            question  answer  \n",
       "0      이것은 무슨 용도입니까?  알 수 없음  \n",
       "1  테이블의 색깔은 무슨 색입니까?       예  \n",
       "2     그린 꽃은 몇 송이입니까?       1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>image</th>\n",
       "      <th>question_id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>81001</td>\n",
       "      <td>dataset/test_images/NIA_dataset03_000000081001...</td>\n",
       "      <td>81001000</td>\n",
       "      <td>다이소는 이 건물 몇층에 위치하고 있습니까?</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81001</td>\n",
       "      <td>dataset/test_images/NIA_dataset03_000000081001...</td>\n",
       "      <td>81001001</td>\n",
       "      <td>도로에 있는 차들은 몇 대입니까?</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>81001</td>\n",
       "      <td>dataset/test_images/NIA_dataset03_000000081001...</td>\n",
       "      <td>81001002</td>\n",
       "      <td>오른쪽 제일 앞에 보이는 차량의 색깔은 무엇입니까?</td>\n",
       "      <td>검정색</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id                                              image  question_id  \\\n",
       "0     81001  dataset/test_images/NIA_dataset03_000000081001...     81001000   \n",
       "1     81001  dataset/test_images/NIA_dataset03_000000081001...     81001001   \n",
       "2     81001  dataset/test_images/NIA_dataset03_000000081001...     81001002   \n",
       "\n",
       "                       question answer  \n",
       "0      다이소는 이 건물 몇층에 위치하고 있습니까?      3  \n",
       "1            도로에 있는 차들은 몇 대입니까?      4  \n",
       "2  오른쪽 제일 앞에 보이는 차량의 색깔은 무엇입니까?    검정색  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>알 수 없음</td>\n",
       "      <td>1855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>예</td>\n",
       "      <td>1508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>아니요</td>\n",
       "      <td>907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>흰색</td>\n",
       "      <td>722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>재킷</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>콘크리트</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>Wii</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>머리띠</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>병 장식</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>419 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     answer  count\n",
       "0    알 수 없음   1855\n",
       "1         예   1508\n",
       "2       아니요    907\n",
       "3        흰색    722\n",
       "4         2    470\n",
       "..      ...    ...\n",
       "414      재킷      1\n",
       "415    콘크리트      1\n",
       "416     Wii      1\n",
       "417     머리띠      1\n",
       "418    병 장식      1\n",
       "\n",
       "[419 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 학습 및 테스트 데이터를 데이터로더 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')  #질의를 처리할 BERT Tokenizer선언\n",
    "#이미지 전처리를 위한 이미지 크기 변환 및 각도조정을 위한 transform 선언\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((356, 356)),\n",
    "        transforms.RandomCrop((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "train_dataset = VQADataset(tokenizer, train_df, train_answer, 30, transform) #학습데이터 전처리\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=100, num_workers=4, shuffle=True, pin_memory=True)\n",
    "test_dataset = VQADataset(tokenizer, test_df, train_answer, 30, transform) #테스트데이터 전처리\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=100, num_workers=4, shuffle=False, pin_memory=True)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 학습을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN LOSS: 0.04737940385341644 Accuracy: 0.1728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 79/100 [00:31<00:08,  2.53it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-abfdb44b2d0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#학습셋을 이용해 100번 학습\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-58422b8d0b14>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#예측된 답변과 실제 정답과 비교하여 lossr계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vqa/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/vqa/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  #학습셋을 이용해 100번 학습\n",
    "    train_fn(model, train_loader, criterion, optimizer, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fn(model, test_loader, test_df, DEVICE)  #test데이터를 이용해 답변예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename=\"model.tar\"):\n",
    "    model.module.bert.save_pretrained(\"./roberta-large-355M\")\n",
    "    tokenizer.save_pretrained(\"./roberta-large-355M\")\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "checkpoint = {\n",
    "    \"state_dict\": model.state_dict(),\n",
    "    \"optimizer\": optimizer.state_dict(),\n",
    "    \"train_df\": train_df,\n",
    "    \"answer_list\": train_answer\n",
    "}\n",
    "save_checkpoint(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = test_df['image'].iloc[900]\n",
    "test_question = test_df['question'].iloc[900]\n",
    "test_answer = test_df['answer'].iloc[900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imshow\n",
    "imshow(np.asarray(Image.open(test_image)))\n",
    "print(test_question, test_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answering(model, test_image, test_question, tokenizer, train_answer, DEVICE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vqa",
   "language": "python",
   "name": "vqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
